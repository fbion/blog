<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>K8s on Shining Moon</title>
    <link>https://blog.monsterxx03.com/tags/k8s/</link>
    <description>Recent content in K8s on Shining Moon</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>monsterxx03</copyright>
    <lastBuildDate>Tue, 11 Sep 2018 15:02:22 +0800</lastBuildDate>
    
	<atom:link href="https://blog.monsterxx03.com/tags/k8s/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>EKS 评测</title>
      <link>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</link>
      <pubDate>Tue, 11 Sep 2018 15:02:22 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/11/eks-%E8%AF%84%E6%B5%8B/</guid>
      <description>EKS 正式 launch 还没有正经用过, 最近总算试了一把, 记录一点.
Setup AWS 官方的 Guide 只提供了一个 cloudformation template 来设置 worker node, 我喜欢用 terraform, 可以跟着这个文档尝试:https://www.terraform.io/docs/providers/aws/guides/eks-getting-started.html 来设置完整的 eks cluster 和管理 worker node 的 autoscaling group.
设置完 EKS 后需要添加一条 ConfigMap:
apiVersion: v1 kind: ConfigMap metadata: name: aws-auth namespace: kube-system data: mapRoles: | - rolearn: arn:aws:iam::&amp;lt;account-id&amp;gt;:role/eksNodeRole username: system:node:{{EC2PrivateDNSName}} groups: - system:bootstrappers - system:nodes  这样 worker node 节点才能加入集群.
网络 之前一直没有在 AWS 上尝试构建 k8s 的一个原因, 就是不喜欢 overlay 网络, 给系统带来了额外的复杂度和管理开销, VPC flowlog 看不到 pod 之间流量, 封包后 tcpdump 不好 debug 应用层流量.</description>
    </item>
    
    <item>
      <title>Kubernetes in Action Notes</title>
      <link>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</link>
      <pubDate>Mon, 03 Sep 2018 18:20:46 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/09/03/kubernetes-in-action-notes/</guid>
      <description>Miscellaneous notes when reading &amp;lt;Kubernetes in Action&amp;gt;.
api group and api version core api group need&amp;rsquo;t specified in apiVersion field.
For example, ReplicationController is on core api group, so only:
apiVersion: v1 kind: ReplicationController ...  ReplicationSet is added later in app group, v1beta2 version (k8s v1.8):
apiVersion: apps/v1beta2 1 kind: ReplicaSet  https://kubernetes.io/docs/concepts/overview/kubernetes-api/
ReplicationController VS ReplicationSet ReplicationController is replaced by ReplicationSet, which has more expressive pod selectors.
ReplicationController&amp;rsquo;s label selector only allows matching pods that include a certain label, ReplicationSet can meet multi labels at same time.</description>
    </item>
    
    <item>
      <title>AWS 的 K8S CNI Plugin</title>
      <link>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</link>
      <pubDate>Mon, 09 Apr 2018 15:28:38 +0800</pubDate>
      
      <guid>https://blog.monsterxx03.com/2018/04/09/aws-%E7%9A%84-k8s-cni-plugin/</guid>
      <description>EKS 还没有 launch, 但 AWS 先开源了自己的 CNI 插件, 简单看了下, 说说它的实现和其他 K8S 网络方案的差别.
K8S 集群对网络有几个基本要求:
 container 之间网络必须可达，且不通过 NAT 所有 node 必须可以和所有 container 通信, 且不通过 NAT container 自己看到的 IP, 必须和其他 container 看到的它的 ip 相同.  Flannel in VPC flannel 是 K8S 的一个 CNI 插件, 在 VPC 里使用 flannel 的话, 有几个选择:
 通过 VXLAN/UDP 进行封包, 封包影响网络性能, 而且不好 debug 用 aws vpc backend, 这种方式会把每台主机的 docker 网段添加进 vpc routing table, 但默认 routing table 里只能有50条规则, 所以只能 50 个 node, 可以发 ticket 提升, 但数量太多会影响 vpc 性能.</description>
    </item>
    
  </channel>
</rss>